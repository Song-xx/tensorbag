{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Boltzman Machines\n",
    "=================\n",
    "\n",
    "A [Restricted Boltzmann machine (RBM)](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. They are a particular form of [Boltzman Machines](https://en.wikipedia.org/wiki/Boltzmann_machine) subject to a **restriction**. The restriction is that there are no connections between nodes within a group of units (meaning that the network form a bipartite graph, see below).\n",
    "\n",
    "The RBM is made of two layers, each one having a certain number of units. The input units are called **visible units** of the RBM because their states are observed. The feature detectors correspond to non-observed **hidden units**. The hidden units are often referred to as **latent variables**, as they do not result directly from the observed data. The two layers are connected through a matrix of weights. The units inside the layer are not connected, meaning that the network form a [bipartite graph](https://en.wikipedia.org/wiki/Bipartite_graph).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"../etc/img/rbm_architecture.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "\n",
    "RBMs were predominantly used in an unsupervised setting to extract better\n",
    "features from the input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy of a configuration\n",
    "-----------------------------\n",
    "\n",
    "The visible and hidden units are often organised as vectors, and a pair of visible-hidden vectors is called a **configuration**. A joint configuration of the visible and hidden units has an **energy** (see Hopfield, 1982) given by:\n",
    "\n",
    "$$E(v,h) = -a^{T}v -b^{T}h -v^{T} Wh $$\n",
    "\n",
    "where the matrix of weights $W$ (size $m \\times n$) associated with the connection between hidden unit $h$ and visible unit $v$, as well as bias weights $a$ for the visible units and $b$ for the hidden units.\n",
    "This definition of Energy is the same used in [Hopefield Networks](https://en.wikipedia.org/wiki/Hopfield_network).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distributions over units\n",
    "------------------------------------------\n",
    "\n",
    "Using the energy it is possible to define a series of probability distribution of both visible and hidden units. Here we consider both $v$ and $h$ as random variables, and they are binary units (there are ways to generalize to real units).\n",
    "\n",
    "The [joint probability](https://en.wikipedia.org/wiki/Joint_probability_distribution) of every possible pair of a visible and a hidden vector can be defined as follows:\n",
    "\n",
    "$$ P(v,h) = \\frac{1}{Z} e^{-E(v,h)}$$\n",
    "\n",
    "where the **partition function** $Z$ is a normalisation factor, given by summing over all possible pairs of visible and hidden vectors. Since both $v$ and $h$ are binary there is an exponential number of values that they can take, and so estimating the partition function is **intractable**. This problem can be managed and we will see how to do it.\n",
    "\n",
    "Using the energy it is also possible to compute the [marginal probability distribution](https://en.wikipedia.org/wiki/Marginal_distribution) on the visible units:\n",
    "\n",
    "$$ P(v) = \\frac{1}{Z} \\sum_{h} e^{-E(v,h)}$$\n",
    "\n",
    "and on the hidden units:\n",
    "\n",
    "$$ P(h) = \\frac{1}{Z} \\sum_{v} e^{-E(v,h)}$$\n",
    "\n",
    "We can also obtain the [conditional probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution) of the $m$ visible units given the hidden units:\n",
    "\n",
    "\n",
    "$$ P(v|h) = \\frac{1}{Z} \\prod_{i=1}^{m} P(v_{i} | h) $$\n",
    "\n",
    "and the of the $n$ hidden units with respect to the visible units:\n",
    "\n",
    "$$ P(h|v) = \\frac{1}{Z} \\prod_{j=1}^{n} P(h_{j} | v) $$\n",
    "\n",
    "In both cases we used the assumption that the visible units are independent given the hidden units (and viceversa).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training objective\n",
    "---------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive divergence\n",
    "-------------------------\n",
    "\n",
    "To solve the problem we can use a method proposed by Hinton and called **Contrastive Divergence (CD)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "------------\n",
    "\n",
    "- A Practical Guide to Training RBMs by Geoffrey Hinton [[pdf]](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "- Notes on Contrastive Divergence [[pdf]](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)\n",
    "- Contrastive Divergence the original article from Hinton [[web]](https://www.mitpressjournals.org/doi/abs/10.1162/089976602760128018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
