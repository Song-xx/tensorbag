{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Pix dataset: downloading and preprocessing\n",
    "=========================================\n",
    "\n",
    "The pix2pix dataset has been released as part of the paper \"Image-to-Image Translation with Conditional Adversarial Networks\" [arxiv](https://arxiv.org/abs/1611.07004) and it contains five different datasets: cityscapes, edges2handbags, edges2shoes, facades, maps. All these datasets are a pixel to pixle conversion from a domain (e.g. satellite images) to another (map images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset\n",
    "------------------------------------\n",
    "\n",
    "As first step we need to download one of the datasets. The following code has been readapted from [this repository](https://github.com/affinelayer/pix2pix-tensorflow) and allows downloading the dataset and extracting the compressed files into local folders. Here, we only download the smallest dataset, but you can uncomment one of the other url if you want to download a different one. The size of each dataset is reported on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\n",
      "Done!\n",
      "Extracting compressed file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import urlopen\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "#dataset_name = \"cityscapes\" #111 MB\n",
    "#dataset_name = \"edges2handbags\" #8.0 GB\n",
    "#dataset_name = \"edges2shoes\" #2.3 GB\n",
    "dataset_name = \"facades\" #31 MB\n",
    "#dataset_name = \"maps\" #253 MB \n",
    "\n",
    "url = \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/\" + dataset_name + \".tar.gz\" #31 MB\n",
    "\n",
    "with tempfile.TemporaryFile() as tmp:\n",
    "    print(\"Downloading dataset from: \" + url)\n",
    "    print(\"Done!\")\n",
    "    shutil.copyfileobj(urlopen(url), tmp)\n",
    "    print(\"Extracting compressed file...\")\n",
    "    tmp.seek(0)\n",
    "    tar = tarfile.open(fileobj=tmp)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing in Numpy\n",
    "------------------------------------\n",
    "\n",
    "Once the dataset has been downloaded it is possible to preprocess it in order to have valid files that we can use in Tensorflow. This snippet generate Numpy files ready to load in memory via the `numpy.load()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images...\n",
      "Dataset (features) shape: (800, 256, 256, 3)\n",
      "Dataset (labels) shape: (800,)\n",
      "Saving numpy files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#dataset_name = \"cityscapes\"\n",
    "#dataset_name = \"edges2handbags\"\n",
    "#dataset_name = \"edges2shoes\"\n",
    "dataset_name = \"facades\"\n",
    "#dataset_name = \"maps\"\n",
    "\n",
    "np_path = \"./\" + dataset_name\n",
    "header = \"./\" + dataset_name + \"/train/\" \n",
    "\n",
    "dataset_features_list = list()\n",
    "dataset_labels_list = list()\n",
    "img_counter = 0\n",
    "for filename in os.listdir(header):\n",
    "    #print(filename) \n",
    "    img_array = np.asarray(Image.open(header + filename))\n",
    "    width = img_array.shape[1]\n",
    "    half_width = int(img_array.shape[1] / 2.0)\n",
    "    first_img_array = img_array[:,0:half_width,:]\n",
    "    second_img_array = img_array[:,half_width:width,:]\n",
    "    dataset_features_list.append(first_img_array)\n",
    "    dataset_labels_list.append(0)\n",
    "    dataset_features_list.append(second_img_array)\n",
    "    dataset_labels_list.append(1)\n",
    "    img_counter += 1\n",
    "    \n",
    "dataset_features_matrix = np.array(dataset_features_list)\n",
    "dataset_labels_array = np.array(dataset_labels_list)\n",
    "print(\"Processed \" + str(img_counter) + \" images...\")\n",
    "print(\"Dataset (features) shape: \" + str(dataset_features_matrix.shape))\n",
    "print(\"Dataset (labels) shape: \" + str(dataset_labels_array.shape))\n",
    "print(\"Saving numpy files...\")\n",
    "np.save(np_path + \"_features\", dataset_features_matrix)\n",
    "np.save(np_path + \"_labels\", dataset_labels_array) \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
