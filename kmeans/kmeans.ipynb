{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering\n",
    "===========\n",
    "\n",
    "The k-means clustering algorithm is an unsupervised learning method broadly used in cluster analysis. the algorithm is based on two main steps:\n",
    "\n",
    "1. **Assignment**: each observation in the input space is assigned to the Best Matching Unit (BMU) based on a distance measure.\n",
    "2. **Update**: for each cluster estimate the mean and assign it to the centroids.\n",
    "\n",
    "In general the distance measure used is th [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). A more efficient version is the [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) that is based on the inner product. The goal in k-means clustering is to minimize the within-cluster sum of squares (reconstruction error):\n",
    "\n",
    "$$\\underset{S}{\\mathrm{argmin}} \\sum_{i=1}^{k} \\sum_{\\bf{x} \\in S_{i}} \\lVert \\bf{x} - \\bf{\\mu}_{i} \\rVert^{2}$$\n",
    "\n",
    "where $\\bf{x}$ are the input vectors, $S = \\{ S_{1},...,S_{k} \\}$ are the number of clusters, and $\\mu$ is the mean of the vectors belonging to each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online k-means algorithm in Tensorflow\n",
    "--------------------------------------------\n",
    "\n",
    "Here I show you how to create an online version of the k-means algorithm. By *online* I mean that a batch of examples can be passed every time the training operation is called. This is particularly useful if the input space is particularly big. First of all we declare the input size and the number of clusters we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "number_centroids = 2 #the number of centroids\n",
    "batch_size = 3 #The number of input vector to pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the real algortihm. The centroids and the input are assigned through two placeholders. The distance used is the **Euclidean distance**. Remember that we want to minimize the within-cluster sum of squares (reconstruction error) between the input vectors and the centroids. Since the problem can be defined in term of a loss function, we can use our dear optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Placeholders for the input array and the initial centroids\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, input_size])\n",
    "initial_centroids_placeholder = tf.placeholder(dtype=tf.float32, shape=[input_size, number_centroids])\n",
    "\n",
    "#Matrix containing the centorids\n",
    "kmeans_matrix = tf.Variable(tf.random_uniform(shape=[input_size, number_centroids], \n",
    "                                              minval=0.0, maxval=1.0, dtype=tf.float32))\n",
    "assign_centroids_op = kmeans_matrix.assign(initial_centroids_placeholder)\n",
    "\n",
    "#Here the distance is estimated and the BMU computed\n",
    "difference = tf.expand_dims(input_placeholder, axis=1) - tf.expand_dims(tf.transpose(kmeans_matrix), axis=0)\n",
    "euclidean_distance = tf.norm(difference, ord='euclidean',axis=2) #shape=(?, 3)\n",
    "bmu_index = tf.argmin(euclidean_distance, axis=1) #get the index of BMU\n",
    "bmu = tf.gather(kmeans_matrix, indices=bmu_index, axis=1) #take the centroinds\n",
    "\n",
    "#To minimize: within-cluster sum of squares (reconstruction error)\n",
    "loss = tf.reduce_mean(tf.pow(input_placeholder - tf.transpose(bmu), 2))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "\n",
    "#Training operation\n",
    "train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graph is ready and we can start a session to try the algorithm. Here I simply create a certain number of random input arrays. The initialization of the centroids is trivial, I take some of the input arrays and I pass them to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_array = np.random.random((batch_size,input_size))\n",
    "print(\"\\ninput_array.T\")\n",
    "print(input_array.T)\n",
    "print(\"\\nkmeans_matrix before assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "sess.run([assign_centroids_op], \n",
    "         {initial_centroids_placeholder: input_array[0:number_centroids,:].T})\n",
    "print(\"\\nkmeans_matrix after assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print_every = 100\n",
    "for i in range(1000):\n",
    "    output = sess.run([train_op, loss], {input_placeholder: input_array})\n",
    "    if(i%print_every==0):\n",
    "        print(\"Loss: \" + str(output[1]))\n",
    "\n",
    "print(\"\\nkmeans_matrix final\")\n",
    "print(sess.run([kmeans_matrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the Iris dataset\n",
    "--------------------------------\n",
    "\n",
    "Now we can try the algorithm on a real dataset. We can use the [Iris Flower dataset](../iris/iris.ipynb) for instance. The dataset is represented by the dimensions of the flowers: sepal length, sepal width, petal length, petal width. There are a total of three classes (0=Setosa, 1=Versicolor, 2=Virginica).\n",
    "The TFRecord files for this dataset are already included in Tensorbag and we can load them in memory with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"feature\": tf.VarLenFeature(tf.float32),\n",
    "                \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    feature = tf.cast(parsed_features[\"feature\"], tf.float32)\n",
    "    feature = tf.sparse_tensor_to_dense(feature, default_value=0)\n",
    "    label = parsed_features[\"label\"]\n",
    "    return feature, label\n",
    "\n",
    "print \"Loading the training datasets...\"\n",
    "tf_train_dataset = tf.data.TFRecordDataset(\"../iris/iris_train.tfrecord\")\n",
    "print \"Parsing the training datasets...\"\n",
    "tf_train_dataset = tf_train_dataset.map(_parse_function)\n",
    "print \"Verifying types and shapes...\"\n",
    "print(tf_train_dataset.output_types)\n",
    "print(tf_train_dataset.output_shapes)\n",
    "print \"Loading the test datasets...\"\n",
    "tf_test_dataset = tf.data.TFRecordDataset(\"../iris/iris_test.tfrecord\")\n",
    "print \"Parsing the test datasets...\"\n",
    "tf_test_dataset = tf_test_dataset.map(_parse_function)\n",
    "print \"Verifying types and shapes...\"\n",
    "print(tf_test_dataset.output_types)\n",
    "print(tf_test_dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train_dataset'):\n",
    "    batch_size = 100\n",
    "    num_epochs = 1000\n",
    "    tf_train_dataset = tf_train_dataset.batch(batch_size)\n",
    "    tf_train_dataset = tf_train_dataset.repeat(num_epochs)\n",
    "    iterator = tf_train_dataset.make_one_shot_iterator()\n",
    "    next_batch_features, next_batch_labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 4 #sepal length, sepal width, petal length, petal width\n",
    "number_centroids = 3 #the number of centroids is equal to the number of the classes\n",
    "\n",
    "#Placeholders for the input array and the initial centroids\n",
    "input_placeholder = next_batch_features\n",
    "initial_centroids_placeholder = tf.placeholder(dtype=tf.float32, shape=[input_size, number_centroids])\n",
    "\n",
    "#Matrix containing the centorids\n",
    "kmeans_matrix = tf.Variable(tf.random_uniform(shape=[input_size, number_centroids], \n",
    "                                              minval=0.0, maxval=1.0, dtype=tf.float32))\n",
    "assign_centroids_op = kmeans_matrix.assign(initial_centroids_placeholder)\n",
    "\n",
    "#Here the distance is estimated and the BMU computed\n",
    "difference = tf.expand_dims(input_placeholder, axis=1) - tf.expand_dims(tf.transpose(kmeans_matrix), axis=0)\n",
    "euclidean_distance = tf.norm(difference, ord='euclidean',axis=2) #shape=(?, 3)\n",
    "bmu_index = tf.argmin(euclidean_distance, axis=1) #get the index of BMU\n",
    "bmu = tf.gather(kmeans_matrix, indices=bmu_index, axis=1) #take the centroinds\n",
    "\n",
    "#To minimize: within-cluster sum of squares (reconstruction error)\n",
    "loss = tf.reduce_mean(tf.pow(input_placeholder - tf.transpose(bmu), 2))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.1)\n",
    "\n",
    "#Training operation\n",
    "train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"\\nkmeans_matrix before assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "input_array = sess.run([next_batch_features])[0]\n",
    "sess.run([assign_centroids_op], \n",
    "         {initial_centroids_placeholder: input_array[0:number_centroids,:].T})\n",
    "print(\"\\nkmeans_matrix after assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "\n",
    "print(\"\\nLoss before training\")\n",
    "output = sess.run([loss])\n",
    "print(output)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "while True:\n",
    "    try:\n",
    "        output = sess.run([train_op, loss, print_op])\n",
    "        print(\"Loss: \" + str(output[1]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "print(\"\\nkmeans_matrix final\")\n",
    "print(sess.run([kmeans_matrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright (c) 2018** Massimiliano Patacchiola, MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
