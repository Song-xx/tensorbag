{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering\n",
    "===========\n",
    "\n",
    "The k-means clustering algorithm is an unsupervised learning method broadly used in cluster analysis. the algorithm is based on two main steps:\n",
    "\n",
    "1. **Assignment**: each observation in the input space is assigned to the Best Matching Unit (BMU) based on a distance measure.\n",
    "2. **Update**: for each cluster estimate the mean and assign it to the centroids.\n",
    "\n",
    "In general the distance measure used is th [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). A more efficient version is the [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) that is based on the inner product. The goal in k-means clustering is to minimize the within-cluster sum of squares (reconstruction error):\n",
    "\n",
    "$$\\underset{S}{\\mathrm{argmin}} \\sum_{i=1}^{k} \\sum_{\\bf{x} \\in S_{i}} \\lVert \\bf{x} - \\bf{\\mu}_{i} \\rVert^{2}$$\n",
    "\n",
    "where $\\bf{x}$ are the input vectors, $S = \\{ S_{1},...,S_{k} \\}$ are the number of clusters, and $\\mu$ is the mean of the vectors belonging to each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online k-means algorithm in Tensorflow\n",
    "--------------------------------------------\n",
    "\n",
    "Here I show you how to create an online version of the k-means algorithm. By *online* I mean that a batch of examples can be passed every time the training operation is called. This is particularly useful if the input space is particularly big. First of all we declare the input size and the number of clusters we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "number_centroids = 2 #the number of centroids\n",
    "batch_size = 3 #The number of input vector to pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the real algortihm. The centroids and the input are assigned through two placeholders. The distance used is the **Euclidean distance**. Remember that we want to minimize the within-cluster sum of squares (reconstruction error) between the input vectors and the centroids. Since the problem can be defined in term of a loss function, we can use our dear optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Placeholders for the input array and the initial centroids\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, input_size])\n",
    "initial_centroids_placeholder = tf.placeholder(dtype=tf.float32, shape=[input_size, number_centroids])\n",
    "\n",
    "#Matrix containing the centorids\n",
    "kmeans_matrix = tf.Variable(tf.random_uniform(shape=[input_size, number_centroids], \n",
    "                                              minval=0.0, maxval=1.0, dtype=tf.float32))\n",
    "assign_centroids_op = kmeans_matrix.assign(initial_centroids_placeholder)\n",
    "\n",
    "#Here the distance is estimated and the BMU computed\n",
    "difference = tf.expand_dims(input_placeholder, axis=1) - tf.expand_dims(tf.transpose(kmeans_matrix), axis=0)\n",
    "euclidean_distance = tf.norm(difference, ord='euclidean',axis=2) #shape=(?, 3)\n",
    "bmu_index = tf.argmin(euclidean_distance, axis=1) #get the index of BMU\n",
    "bmu = tf.gather(kmeans_matrix, indices=bmu_index, axis=1) #take the centroinds\n",
    "\n",
    "#To minimize: within-cluster sum of squares (reconstruction error)\n",
    "loss = tf.reduce_mean(tf.pow(input_placeholder - tf.transpose(bmu), 2))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "\n",
    "#Training operation\n",
    "train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graph is ready and we can start a session to try the algorithm. Here I simply create a certain number of random input arrays. The initialization of the centroids is trivial, I take some of the input arrays and I pass them to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_array = np.random.random((batch_size,input_size))\n",
    "print(\"\\ninput_array.T\")\n",
    "print(input_array.T)\n",
    "print(\"\\nkmeans_matrix before assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "sess.run([assign_centroids_op], \n",
    "         {initial_centroids_placeholder: input_array[0:number_centroids,:].T})\n",
    "print(\"\\nkmeans_matrix after assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print_every = 100\n",
    "for i in range(1000):\n",
    "    output = sess.run([train_op, loss], {input_placeholder: input_array})\n",
    "    if(i%print_every==0):\n",
    "        print(\"Loss: \" + str(output[1]))\n",
    "\n",
    "print(\"\\nkmeans_matrix final\")\n",
    "print(sess.run([kmeans_matrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the Iris dataset\n",
    "--------------------------------\n",
    "\n",
    "Now we can try the algorithm on a real dataset. We can use the [Iris Flower dataset](../iris/iris.ipynb) for instance. The dataset is represented by the dimensions of the flowers: sepal length, sepal width, petal length, petal width. There are a total of three classes (0=Setosa, 1=Versicolor, 2=Virginica).\n",
    "The TFRecord files for this dataset are already included in Tensorbag and we can load them in memory with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training datasets...\n",
      "Parsing the training datasets...\n",
      "Verifying types and shapes...\n",
      "(tf.float32, tf.int64)\n",
      "(TensorShape([Dimension(None)]), TensorShape([]))\n",
      "Loading the test datasets...\n",
      "Parsing the test datasets...\n",
      "Verifying types and shapes...\n",
      "(tf.float32, tf.int64)\n",
      "(TensorShape([Dimension(None)]), TensorShape([]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"feature\": tf.VarLenFeature(tf.float32),\n",
    "                \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    feature = tf.cast(parsed_features[\"feature\"], tf.float32)\n",
    "    feature = tf.sparse_tensor_to_dense(feature, default_value=0)\n",
    "    label = parsed_features[\"label\"]\n",
    "    return feature, label\n",
    "\n",
    "print \"Loading the training datasets...\"\n",
    "tf_train_dataset = tf.data.TFRecordDataset(\"../iris/iris_train.tfrecord\")\n",
    "print \"Parsing the training datasets...\"\n",
    "tf_train_dataset = tf_train_dataset.map(_parse_function)\n",
    "print \"Verifying types and shapes...\"\n",
    "print(tf_train_dataset.output_types)\n",
    "print(tf_train_dataset.output_shapes)\n",
    "print \"Loading the test datasets...\"\n",
    "tf_test_dataset = tf.data.TFRecordDataset(\"../iris/iris_test.tfrecord\")\n",
    "print \"Parsing the test datasets...\"\n",
    "tf_test_dataset = tf_test_dataset.map(_parse_function)\n",
    "print \"Verifying types and shapes...\"\n",
    "print(tf_test_dataset.output_types)\n",
    "print(tf_test_dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train_dataset'):\n",
    "    batch_size = 100\n",
    "    num_epochs = 1000\n",
    "    tf_train_dataset = tf_train_dataset.batch(batch_size)\n",
    "    tf_train_dataset = tf_train_dataset.repeat(num_epochs)\n",
    "    iterator = tf_train_dataset.make_one_shot_iterator()\n",
    "    next_batch_features, next_batch_labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 4 #sepal length, sepal width, petal length, petal width\n",
    "number_centroids = 3 #the number of centroids is equal to the number of the classes\n",
    "\n",
    "#Placeholders for the input array and the initial centroids\n",
    "input_placeholder = next_batch_features\n",
    "initial_centroids_placeholder = tf.placeholder(dtype=tf.float32, shape=[input_size, number_centroids])\n",
    "\n",
    "#Matrix containing the centorids\n",
    "kmeans_matrix = tf.Variable(tf.random_uniform(shape=[input_size, number_centroids], \n",
    "                                              minval=0.0, maxval=1.0, dtype=tf.float32))\n",
    "assign_centroids_op = kmeans_matrix.assign(initial_centroids_placeholder)\n",
    "\n",
    "#Here the distance is estimated and the BMU computed\n",
    "difference = tf.expand_dims(input_placeholder, axis=1) - tf.expand_dims(tf.transpose(kmeans_matrix), axis=0)\n",
    "euclidean_distance = tf.norm(difference, ord='euclidean',axis=2) #shape=(?, 3)\n",
    "bmu_index = tf.argmin(euclidean_distance, axis=1) #get the index of BMU\n",
    "bmu = tf.gather(kmeans_matrix, indices=bmu_index, axis=1) #take the centroinds\n",
    "\n",
    "#To minimize: within-cluster sum of squares (reconstruction error)\n",
    "loss = tf.reduce_mean(tf.pow(input_placeholder - tf.transpose(bmu), 2))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.1)\n",
    "\n",
    "#Training operation\n",
    "train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kmeans_matrix before assignment\n",
      "[array([[0.94429076, 0.33206284, 0.8901272 ],\n",
      "       [0.24301016, 0.5296408 , 0.25928485],\n",
      "       [0.9749198 , 0.8789613 , 0.9710479 ],\n",
      "       [0.28901565, 0.8512291 , 0.80947495]], dtype=float32)]\n",
      "\n",
      "kmeans_matrix after assignment\n",
      "[array([[5.5, 4.9, 6.9],\n",
      "       [3.5, 3. , 3.1],\n",
      "       [1.3, 1.4, 5.1],\n",
      "       [0.2, 0.2, 2.3]], dtype=float32)]\n",
      "\n",
      "Loss before training\n",
      "[0.3781]\n",
      "\n",
      "Starting training...\n",
      "Loss: 0.3781\n",
      "Loss: 0.3708525\n",
      "Loss: 0.36320505\n",
      "Loss: 0.3556881\n",
      "Loss: 0.3483276\n",
      "Loss: 0.34114778\n",
      "Loss: 0.33417183\n",
      "Loss: 0.32742205\n",
      "Loss: 0.32091957\n",
      "Loss: 0.31468362\n",
      "Loss: 0.3087318\n",
      "Loss: 0.30307946\n",
      "Loss: 0.29773933\n",
      "Loss: 0.2927216\n",
      "Loss: 0.28803366\n",
      "Loss: 0.2836796\n",
      "Loss: 0.27966052\n",
      "Loss: 0.27597445\n",
      "Loss: 0.27261636\n",
      "Loss: 0.26957813\n",
      "Loss: 0.2668487\n",
      "Loss: 0.26441464\n",
      "Loss: 0.26226008\n",
      "Loss: 0.2603674\n",
      "Loss: 0.25871715\n",
      "Loss: 0.25728917\n",
      "Loss: 0.25594294\n",
      "Loss: 0.25453267\n",
      "Loss: 0.25335968\n",
      "Loss: 0.25239164\n",
      "Loss: 0.25159797\n",
      "Loss: 0.25095072\n",
      "Loss: 0.25042483\n",
      "Loss: 0.24999787\n",
      "Loss: 0.24965078\n",
      "Loss: 0.24936731\n",
      "Loss: 0.24913427\n",
      "Loss: 0.24894074\n",
      "Loss: 0.24877834\n",
      "Loss: 0.24864058\n",
      "Loss: 0.24852242\n",
      "Loss: 0.24842052\n",
      "Loss: 0.24833202\n",
      "Loss: 0.24825504\n",
      "Loss: 0.24818815\n",
      "Loss: 0.2481303\n",
      "Loss: 0.2480804\n",
      "Loss: 0.24803782\n",
      "Loss: 0.24800171\n",
      "Loss: 0.24797158\n",
      "Loss: 0.24794652\n",
      "Loss: 0.24792415\n",
      "Loss: 0.24788357\n",
      "Loss: 0.24785179\n",
      "Loss: 0.24782738\n",
      "Loss: 0.24780884\n",
      "Loss: 0.24779515\n",
      "Loss: 0.24778515\n",
      "Loss: 0.24777807\n",
      "Loss: 0.2477731\n",
      "Loss: 0.2477697\n",
      "Loss: 0.24776752\n",
      "Loss: 0.24776603\n",
      "Loss: 0.24776518\n",
      "Loss: 0.24776463\n",
      "Loss: 0.2477643\n",
      "Loss: 0.2477641\n",
      "Loss: 0.24776404\n",
      "Loss: 0.24776399\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776393\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776393\n",
      "Loss: 0.2477639\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776392\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776392\n",
      "Loss: 0.24776392\n",
      "Loss: 0.24776392\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776389\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776392\n",
      "Loss: 0.24776392\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776392\n",
      "Loss: 0.2477639\n",
      "Loss: 0.24776393\n",
      "Loss: 0.24776405\n",
      "Loss: 0.24776556\n",
      "Loss: 0.24778457\n",
      "Loss: 0.24806003\n",
      "Loss: 0.25131845\n",
      "Loss: 0.25862497\n",
      "Loss: 0.2538802\n",
      "Loss: 0.2494223\n",
      "Loss: 0.2482119\n",
      "Loss: 0.24791451\n",
      "Loss: 0.24782924\n",
      "Loss: 0.2478004\n",
      "Loss: 0.24778968\n",
      "Loss: 0.24778679\n",
      "Loss: 0.24778897\n",
      "Loss: 0.2477975\n",
      "Loss: 0.24781834\n",
      "Loss: 0.2478691\n",
      "Loss: 0.24800135\n",
      "Loss: 0.2483634\n",
      "Loss: 0.2492886\n",
      "Loss: 0.25091267\n",
      "Loss: 0.25187337\n",
      "Loss: 0.25101703\n",
      "Loss: 0.25011978\n",
      "Loss: 0.25182983\n",
      "Loss: 0.25316727\n",
      "Loss: 0.25171378\n",
      "Loss: 0.25036976\n",
      "Loss: 0.2505786\n",
      "Loss: 0.24912448\n",
      "Loss: 0.24900347\n",
      "Loss: 0.24927768\n",
      "Loss: 0.24958208\n",
      "Loss: 0.24988605\n",
      "Loss: 0.25026667\n",
      "Loss: 0.25057483\n",
      "Loss: 0.25054753\n",
      "Loss: 0.25011945\n",
      "Loss: 0.24954744\n",
      "Loss: 0.24912022\n",
      "Loss: 0.24892712\n",
      "Loss: 0.2490142\n",
      "Loss: 0.24930447\n",
      "Loss: 0.24984501\n",
      "Loss: 0.25043067\n",
      "Loss: 0.26277244\n",
      "Loss: 0.25092325\n",
      "Loss: 0.25063527\n",
      "Loss: 0.24996653\n",
      "Loss: 0.24966301\n",
      "Loss: 0.24960396\n",
      "Loss: 0.25011474\n",
      "Loss: 0.250955\n",
      "Loss: 0.25101826\n",
      "Loss: 0.2501365\n",
      "Loss: 0.24940911\n",
      "Loss: 0.2490362\n",
      "Loss: 0.24895771\n",
      "Loss: 0.24916\n",
      "Loss: 0.24952684\n",
      "Loss: 0.24959499\n",
      "Loss: 0.24972515\n",
      "Loss: 0.25002655\n",
      "Loss: 0.25025928\n",
      "Loss: 0.25036594\n",
      "Loss: 0.25057715\n",
      "Loss: 0.2506054\n",
      "Loss: 0.2503234\n",
      "Loss: 0.24995066\n",
      "Loss: 0.24968207\n",
      "Loss: 0.24994388\n",
      "Loss: 0.25187147\n",
      "Loss: 0.25174758\n",
      "Loss: 0.25048518\n",
      "Loss: 0.25018537\n",
      "Loss: 0.25005805\n",
      "Loss: 0.25004056\n",
      "Loss: 0.25011614\n",
      "Loss: 0.250194\n",
      "Loss: 0.25023976\n",
      "Loss: 0.25021434\n",
      "Loss: 0.2500955\n",
      "Loss: 0.24993125\n",
      "Loss: 0.24979876\n",
      "Loss: 0.24977173\n",
      "Loss: 0.24993943\n",
      "Loss: 0.25037506\n",
      "Loss: 0.25085884\n",
      "Loss: 0.25092256\n",
      "Loss: 0.25067133\n",
      "Loss: 0.25043842\n",
      "Loss: 0.25030717\n",
      "Loss: 0.25023943\n",
      "Loss: 0.2501901\n",
      "Loss: 0.25013268\n",
      "Loss: 0.25006458\n",
      "Loss: 0.25000453\n",
      "Loss: 0.24998155\n",
      "Loss: 0.2500241\n",
      "Loss: 0.25014815\n",
      "Loss: 0.2503312\n",
      "Loss: 0.2504929\n",
      "Loss: 0.25054976\n",
      "Loss: 0.2505031\n",
      "Loss: 0.25041458\n",
      "Loss: 0.25033042\n",
      "Loss: 0.25026292\n",
      "Loss: 0.25020823\n",
      "Loss: 0.25016278\n",
      "Loss: 0.25012884\n",
      "Loss: 0.25011402\n",
      "Loss: 0.25012726\n",
      "Loss: 0.2501726\n",
      "Loss: 0.25024334\n",
      "Loss: 0.25031912\n",
      "Loss: 0.25037366\n",
      "Loss: 0.250391\n",
      "Loss: 0.2503743\n",
      "Loss: 0.25033826\n",
      "Loss: 0.25029677\n",
      "Loss: 0.25025794\n",
      "Loss: 0.2502252\n",
      "Loss: 0.2502011\n",
      "Loss: 0.25018835\n",
      "Loss: 0.25018948\n",
      "Loss: 0.25020564\n",
      "Loss: 0.25023416\n",
      "Loss: 0.25026843\n",
      "Loss: 0.25029927\n",
      "Loss: 0.25031856\n",
      "Loss: 0.25032273\n",
      "Loss: 0.2503138\n",
      "Loss: 0.25029638\n",
      "Loss: 0.2502756\n",
      "Loss: 0.25025555\n",
      "Loss: 0.25023925\n",
      "Loss: 0.25022873\n",
      "Loss: 0.2502254\n",
      "Loss: 0.25022975\n",
      "Loss: 0.25024077\n",
      "Loss: 0.250256\n",
      "Loss: 0.25027177\n",
      "Loss: 0.25028437\n",
      "Loss: 0.2502913\n",
      "Loss: 0.25029153\n",
      "Loss: 0.250286\n",
      "Loss: 0.25027695\n",
      "Loss: 0.25026655\n",
      "Loss: 0.25025678\n",
      "Loss: 0.25024933\n",
      "Loss: 0.2502454\n",
      "Loss: 0.25024533\n",
      "Loss: 0.25024894\n",
      "Loss: 0.25025535\n",
      "Loss: 0.25026304\n",
      "Loss: 0.25027013\n",
      "Loss: 0.25027522\n",
      "Loss: 0.2502774\n",
      "Loss: 0.25027665\n",
      "Loss: 0.25027344\n",
      "Loss: 0.2502687\n",
      "Loss: 0.2502635\n",
      "Loss: 0.250259\n",
      "Loss: 0.25025585\n",
      "Loss: 0.25025463\n",
      "Loss: 0.25025532\n",
      "Loss: 0.2502576\n",
      "Loss: 0.25026104\n",
      "Loss: 0.25026473\n",
      "Loss: 0.2502679\n",
      "Loss: 0.25026998\n",
      "Loss: 0.25027055\n",
      "Loss: 0.25026968\n",
      "Loss: 0.2502678\n",
      "Loss: 0.25026542\n",
      "Loss: 0.25026295\n",
      "Loss: 0.2502609\n",
      "Loss: 0.25025967\n",
      "Loss: 0.25025937\n",
      "Loss: 0.25026\n",
      "Loss: 0.2502614\n",
      "Loss: 0.25026312\n",
      "Loss: 0.25026485\n",
      "Loss: 0.2502663\n",
      "Loss: 0.250267\n",
      "Loss: 0.25026712\n",
      "Loss: 0.2502665\n",
      "Loss: 0.25026548\n",
      "Loss: 0.2502642\n",
      "Loss: 0.25026304\n",
      "Loss: 0.25026217\n",
      "Loss: 0.2502617\n",
      "Loss: 0.25026175\n",
      "Loss: 0.25026223\n",
      "Loss: 0.25026298\n",
      "Loss: 0.25026384\n",
      "Loss: 0.25026467\n",
      "Loss: 0.25026527\n",
      "Loss: 0.25026548\n",
      "Loss: 0.2502654\n",
      "Loss: 0.25026503\n",
      "Loss: 0.25026447\n",
      "Loss: 0.2502638\n",
      "Loss: 0.2502633\n",
      "Loss: 0.25026298\n",
      "Loss: 0.25026283\n",
      "Loss: 0.25026295\n",
      "Loss: 0.25026324\n",
      "Loss: 0.2502636\n",
      "Loss: 0.25026402\n",
      "Loss: 0.25026444\n",
      "Loss: 0.25026459\n",
      "Loss: 0.25026467\n",
      "Loss: 0.25026459\n",
      "Loss: 0.25026438\n",
      "Loss: 0.25026405\n",
      "Loss: 0.25026375\n",
      "Loss: 0.2502635\n",
      "Loss: 0.2502634\n",
      "Loss: 0.2502634\n",
      "Loss: 0.2502635\n",
      "Loss: 0.25026366\n",
      "Loss: 0.2502638\n",
      "Loss: 0.25026402\n",
      "Loss: 0.25026417\n",
      "Loss: 0.2502643\n",
      "Loss: 0.2502643\n",
      "Loss: 0.25026426\n",
      "Loss: 0.25026408\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502638\n",
      "Loss: 0.25026366\n",
      "Loss: 0.25026366\n",
      "Loss: 0.25026366\n",
      "Loss: 0.25026372\n",
      "Loss: 0.25026375\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026405\n",
      "Loss: 0.25026405\n",
      "Loss: 0.25026414\n",
      "Loss: 0.25026414\n",
      "Loss: 0.25026402\n",
      "Loss: 0.25026405\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502638\n",
      "Loss: 0.2502638\n",
      "Loss: 0.2502638\n",
      "Loss: 0.25026378\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.250264\n",
      "Loss: 0.25026405\n",
      "Loss: 0.25026402\n",
      "Loss: 0.250264\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026384\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026384\n",
      "Loss: 0.25026384\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026402\n",
      "Loss: 0.250264\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026387\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026396\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.2502639\n",
      "Loss: 0.2502639\n",
      "Loss: 0.25026387\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "Loss: 0.25026393\n",
      "\n",
      "kmeans_matrix final\n",
      "[array([[5.220588  , 4.7357144 , 6.3419356 ],\n",
      "       [3.738235  , 3.197619  , 2.914516  ],\n",
      "       [1.5558822 , 1.5785716 , 4.866129  ],\n",
      "       [0.3088234 , 0.29761893, 1.6354839 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"\\nkmeans_matrix before assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "input_array = sess.run([next_batch_features])[0]\n",
    "sess.run([assign_centroids_op], \n",
    "         {initial_centroids_placeholder: input_array[0:number_centroids,:].T})\n",
    "print(\"\\nkmeans_matrix after assignment\")\n",
    "print(sess.run([kmeans_matrix]))\n",
    "\n",
    "print(\"\\nLoss before training\")\n",
    "output = sess.run([loss])\n",
    "print(output)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "while True:\n",
    "    try:\n",
    "        output = sess.run([train_op, loss, print_op])\n",
    "        print(\"Loss: \" + str(output[1]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "print(\"\\nkmeans_matrix final\")\n",
    "print(sess.run([kmeans_matrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright (c) 2018** Massimiliano Patacchiola, MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
